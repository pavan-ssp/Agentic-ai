{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd658e0",
   "metadata": {},
   "source": [
    "#### Stemming and Lemmatization :\n",
    "\n",
    "Stemming is a process that gives similar kind of words from a group of words.\n",
    "\n",
    "Eg: history,historical---> histori\n",
    "\n",
    "\n",
    "     finally,finaliza,final---> fina\n",
    "\n",
    "\n",
    "Lemmatization is also works as like stemming but it gives proper meaningful words.\n",
    "\n",
    "Eg: history,historical---> historical\n",
    "\n",
    "\n",
    "     finally,finaliza,final---> finaly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5eb2dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d993443",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragrah=\"\"\"\n",
    "Natural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages.\n",
    "It involves various tasks, such as text analysis, sentiment analysis, and machine translation.\n",
    "One of the most common NLP tasks is tokenization, which is the process of splitting text into smaller units.\n",
    "These units can be words or sentences. In tokenization, the text is divided into sentences using sentence tokenization,\n",
    "and each sentence is further broken down into individual words using word tokenization.\n",
    "Tokenization is a crucial step for many other NLP applications, including text classification and named entity recognition.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aaeb5e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences=nltk.sent_tokenize(paragrah)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f186971c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nNatural Language Processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human languages.',\n",
       " 'It involves various tasks, such as text analysis, sentiment analysis, and machine translation.',\n",
       " 'One of the most common NLP tasks is tokenization, which is the process of splitting text into smaller units.',\n",
       " 'These units can be words or sentences.',\n",
       " 'In tokenization, the text is divided into sentences using sentence tokenization,\\nand each sentence is further broken down into individual words using word tokenization.',\n",
       " 'Tokenization is a crucial step for many other NLP applications, including text classification and named entity recognition.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8e4b658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Natural',\n",
       " 'Language',\n",
       " 'Processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'field',\n",
       " 'of',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'that',\n",
       " 'focuses',\n",
       " 'on',\n",
       " 'the',\n",
       " 'interaction',\n",
       " 'between',\n",
       " 'computers',\n",
       " 'and',\n",
       " 'human',\n",
       " 'languages',\n",
       " '.',\n",
       " 'It',\n",
       " 'involves',\n",
       " 'various',\n",
       " 'tasks',\n",
       " ',',\n",
       " 'such',\n",
       " 'as',\n",
       " 'text',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'sentiment',\n",
       " 'analysis',\n",
       " ',',\n",
       " 'and',\n",
       " 'machine',\n",
       " 'translation',\n",
       " '.',\n",
       " 'One',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'common',\n",
       " 'NLP',\n",
       " 'tasks',\n",
       " 'is',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'which',\n",
       " 'is',\n",
       " 'the',\n",
       " 'process',\n",
       " 'of',\n",
       " 'splitting',\n",
       " 'text',\n",
       " 'into',\n",
       " 'smaller',\n",
       " 'units',\n",
       " '.',\n",
       " 'These',\n",
       " 'units',\n",
       " 'can',\n",
       " 'be',\n",
       " 'words',\n",
       " 'or',\n",
       " 'sentences',\n",
       " '.',\n",
       " 'In',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'the',\n",
       " 'text',\n",
       " 'is',\n",
       " 'divided',\n",
       " 'into',\n",
       " 'sentences',\n",
       " 'using',\n",
       " 'sentence',\n",
       " 'tokenization',\n",
       " ',',\n",
       " 'and',\n",
       " 'each',\n",
       " 'sentence',\n",
       " 'is',\n",
       " 'further',\n",
       " 'broken',\n",
       " 'down',\n",
       " 'into',\n",
       " 'individual',\n",
       " 'words',\n",
       " 'using',\n",
       " 'word',\n",
       " 'tokenization',\n",
       " '.',\n",
       " 'Tokenization',\n",
       " 'is',\n",
       " 'a',\n",
       " 'crucial',\n",
       " 'step',\n",
       " 'for',\n",
       " 'many',\n",
       " 'other',\n",
       " 'NLP',\n",
       " 'applications',\n",
       " ',',\n",
       " 'including',\n",
       " 'text',\n",
       " 'classification',\n",
       " 'and',\n",
       " 'named',\n",
       " 'entity',\n",
       " 'recognition',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize=nltk.word_tokenize(paragrah)\n",
    "word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c594a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c3a307e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11762204",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb47be6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "  words=nltk.word_tokenize(sentences[i])\n",
    "  words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "  sentences[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "589cc3ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natur languag process ( nlp ) field artifici intellig focus interact comput human languag .',\n",
       " 'it involv variou task , text analysi , sentiment analysi , machin translat .',\n",
       " 'one common nlp task token , process split text smaller unit .',\n",
       " 'these unit word sentenc .',\n",
       " 'in token , text divid sentenc use sentenc token , sentenc broken individu word use word token .',\n",
       " 'token crucial step mani nlp applic , includ text classif name entiti recognit .']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "759289f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\chand\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3c59665",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d62764e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences)):\n",
    "  words=nltk.word_tokenize(sentences[i])\n",
    "  words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "  sentences[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f7fcac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natur languag process ( nlp ) field artifici intellig focus interact comput human languag .',\n",
       " 'involv variou task , text analysi , sentiment analysi , machin translat .',\n",
       " 'one common nlp task token , process split text smaller unit .',\n",
       " 'unit word sentenc .',\n",
       " 'token , text divid sentenc use sentenc token , sentenc broken individu word use word token .',\n",
       " 'token crucial step mani nlp applic , includ text classif name entiti recognit .']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticaienv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
